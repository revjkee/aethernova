# File: neuroforge-core/ops/docker/docker-compose.yaml
# Compose v3.9 — промышленная оркестрация для neuroforge-core
# Требуется: Docker 24+, Docker Compose v2+
version: "3.9"

x-common-env: &common-env
  ENVIRONMENT: ${ENVIRONMENT:-development}
  APP_NAME: ${APP_NAME:-neuroforge-core}
  APP_VERSION: ${APP_VERSION:-0.1.0}
  TZ: ${TZ:-UTC}
  OTEL_EXPORTER_OTLP_ENDPOINT: ${OTEL_EXPORTER_OTLP_ENDPOINT:-http://otel-collector:4318}
  OTEL_SERVICE_NAME: ${APP_NAME:-neuroforge-core}
  LOG_LEVEL: ${LOG_LEVEL:-INFO}
  SENTRY_DSN: ${SENTRY_DSN:-}

x-common-deploy: &common-deploy
  restart: unless-stopped
  security_opt:
    - no-new-privileges:true
  ulimits:
    nofile:
      soft: 65536
      hard: 65536
  logging:
    driver: "json-file"
    options:
      max-size: "10m"
      max-file: "5"

x-health-postgres: &health-postgres
  test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-neuroforge} -d ${POSTGRES_DB:-neuroforge} -h localhost"]
  interval: 10s
  timeout: 5s
  retries: 10
  start_period: 20s

x-health-redis: &health-redis
  test: ["CMD", "redis-cli", "ping"]
  interval: 5s
  timeout: 3s
  retries: 20
  start_period: 10s

x-health-minio: &health-minio
  test: ["CMD-SHELL", "curl -fsS http://localhost:9000/minio/health/ready || exit 1"]
  interval: 10s
  timeout: 5s
  retries: 20
  start_period: 20s

x-health-nginx: &health-nginx
  test: ["CMD-SHELL", "wget -q -O- http://localhost:8080/healthz || exit 1"]
  interval: 10s
  timeout: 5s
  retries: 12
  start_period: 20s

x-core-env: &core-env
  <<: *common-env
  # Бэкенд-конфигурация приложения (совместимо с .env.example)
  POSTGRES_DSN: ${POSTGRES_DSN:-postgresql://neuroforge:${POSTGRES_PASSWORD}@postgres:5432/neuroforge?sslmode=disable}
  REDIS_DSN: ${REDIS_DSN:-redis://redis:6379/1}
  S3_ENABLED: ${S3_ENABLED:-true}
  S3_ENDPOINT: ${S3_ENDPOINT:-http://minio:9000}
  S3_REGION: ${S3_REGION:-us-east-1}
  S3_ACCESS_KEY_ID: ${S3_ACCESS_KEY_ID:-minioadmin}
  S3_SECRET_ACCESS_KEY: ${S3_SECRET_ACCESS_KEY:-minioadmin}
  S3_BUCKET_MODELS: ${S3_BUCKET_MODELS:-neuroforge-models}
  S3_BUCKET_ARTIFACTS: ${S3_BUCKET_ARTIFACTS:-neuroforge-artifacts}
  QUEUE_BACKEND: ${QUEUE_BACKEND:-celery}
  CELERY_BROKER_URL: ${CELERY_BROKER_URL:-redis://redis:6379/3}
  CELERY_RESULT_BACKEND: ${CELERY_RESULT_BACKEND:-redis://redis:6379/4}
  PROMETHEUS_ENABLED: ${PROMETHEUS_ENABLED:-true}
  PROMETHEUS_SCRAPE_PATH: ${PROMETHEUS_SCRAPE_PATH:-/metrics}
  CORS_ENABLED: ${CORS_ENABLED:-true}
  HTTP_PORT: ${HTTP_PORT:-8080}

networks:
  edge:
    driver: bridge
  backend:
    driver: bridge
    internal: true
  observability:
    driver: bridge

volumes:
  pg_data:
  redis_data:
  minio_data:
  minio_config:
  app_var:
  grafana_data:
  prometheus_data:
  loki_data:

secrets:
  postgres_password:
    file: ./secrets/postgres_password.txt
  s3_access_key:
    file: ./secrets/minio_access_key.txt
  s3_secret_key:
    file: ./secrets/minio_secret_key.txt

services:

  postgres:
    image: postgres:16-alpine
    container_name: postgres
    command:
      - "postgres"
      - "-c"
      - "max_connections=300"
      - "-c"
      - "shared_buffers=256MB"
      - "-c"
      - "work_mem=16MB"
      - "-c"
      - "wal_compression=on"
    environment:
      POSTGRES_DB: ${POSTGRES_DB:-neuroforge}
      POSTGRES_USER: ${POSTGRES_USER:-neuroforge}
      POSTGRES_PASSWORD_FILE: /run/secrets/postgres_password
      TZ: ${TZ:-UTC}
    secrets:
      - postgres_password
    healthcheck: *health-postgres
    deploy:
      resources:
        limits:
          cpus: "1.50"
          memory: 2g
        reservations:
          cpus: "0.25"
          memory: 256m
    volumes:
      - pg_data:/var/lib/postgresql/data
    networks:
      - backend
    <<: *common-deploy

  redis:
    image: redis:7-alpine
    container_name: redis
    command: ["redis-server", "--save", "", "--appendonly", "no"]
    healthcheck: *health-redis
    deploy:
      resources:
        limits:
          cpus: "0.75"
          memory: 512m
        reservations:
          cpus: "0.10"
          memory: 64m
    volumes:
      - redis_data:/data
    networks:
      - backend
    <<: *common-deploy

  minio:
    image: quay.io/minio/minio:RELEASE.2025-02-20T00-00-00Z
    container_name: minio
    command: ["server", "/data", "--console-address", ":9001"]
    environment:
      MINIO_ROOT_USER_FILE: /run/secrets/s3_access_key
      MINIO_ROOT_PASSWORD_FILE: /run/secrets/s3_secret_key
      MINIO_BROWSER_REDIRECT_URL: http://localhost:9001
    secrets:
      - s3_access_key
      - s3_secret_key
    ports:
      - "9000:9000"
      - "9001:9001"
    healthcheck: *health-minio
    volumes:
      - minio_data:/data
      - minio_config:/root/.minio
    deploy:
      resources:
        limits:
          cpus: "1.00"
          memory: 1g
        reservations:
          cpus: "0.10"
          memory: 128m
    networks:
      - backend
      - edge
    <<: *common-deploy
    profiles: ["dev","infra","prod"]

  app:
    image: ghcr.io/your-org/neuroforge-core:${APP_IMAGE_TAG:-latest}
    container_name: app
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      minio:
        condition: service_healthy
    environment:
      <<: *core-env
      # Если используете Docker secrets для S3, перегружаем:
      S3_ACCESS_KEY_ID_FILE: /run/secrets/s3_access_key
      S3_SECRET_ACCESS_KEY_FILE: /run/secrets/s3_secret_key
    secrets:
      - s3_access_key
      - s3_secret_key
    healthcheck:
      test: ["CMD-SHELL", "wget -q -O- http://localhost:${HTTP_PORT:-8080}/healthz || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 12
      start_period: 20s
    ports:
      - "${HTTP_PORT:-8080}:8080"
    volumes:
      - app_var:/app/var
    deploy:
      resources:
        limits:
          cpus: "2.00"
          memory: 2g
        reservations:
          cpus: "0.50"
          memory: 512m
    networks:
      - backend
      - edge
    <<: *common-deploy

  worker:
    image: ghcr.io/your-org/neuroforge-core:${APP_IMAGE_TAG:-latest}
    container_name: worker
    command: ["python", "-m", "neuroforge.worker.celery", "worker", "-O", "fair", "--loglevel=${LOG_LEVEL:-INFO}"]
    environment:
      <<: *core-env
    depends_on:
      redis:
        condition: service_healthy
      app:
        condition: service_started
    deploy:
      resources:
        limits:
          cpus: "1.50"
          memory: 1g
        reservations:
          cpus: "0.25"
          memory: 256m
    networks:
      - backend
    <<: *common-deploy

  scheduler:
    image: ghcr.io/your-org/neuroforge-core:${APP_IMAGE_TAG:-latest}
    container_name: scheduler
    command: ["python", "-m", "neuroforge.worker.celery", "beat", "--loglevel=${LOG_LEVEL:-INFO}"]
    environment:
      <<: *core-env
    depends_on:
      redis:
        condition: service_healthy
    deploy:
      resources:
        limits:
          cpus: "0.50"
          memory: 512m
        reservations:
          cpus: "0.10"
          memory: 128m
    networks:
      - backend
    <<: *common-deploy

  # Обратный прокси (вариант NGINX). Профиль edge.
  nginx:
    image: nginx:1.27-alpine
    container_name: nginx
    depends_on:
      app:
        condition: service_healthy
    ports:
      - "${EDGE_HTTP_PORT:-80}:8080"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
    environment:
      TZ: ${TZ:-UTC}
    healthcheck: *health-nginx
    deploy:
      resources:
        limits:
          cpus: "0.50"
          memory: 256m
        reservations:
          cpus: "0.05"
          memory: 64m
    networks:
      - edge
      - backend
    <<: *common-deploy
    profiles: ["edge"]

  # -------- Наблюдаемость (включайте профилем "observability") --------------

  otel-collector:
    image: otel/opentelemetry-collector:0.104.0
    container_name: otel-collector
    command: ["--config=/etc/otelcol/config.yaml"]
    volumes:
      - ./observability/otel-collector.yaml:/etc/otelcol/config.yaml:ro
    ports:
      - "4317:4317"  # OTLP gRPC
      - "4318:4318"  # OTLP HTTP
    networks:
      - observability
      - backend
    <<: *common-deploy
    profiles: ["observability"]

  prometheus:
    image: prom/prometheus:v2.53.0
    container_name: prometheus
    volumes:
      - ./observability/prometheus.yaml:/etc/prometheus/prometheus.yml:ro
      - prometheus_data:/prometheus
    command: ["--config.file=/etc/prometheus/prometheus.yml", "--storage.tsdb.retention.time=15d"]
    ports:
      - "9090:9090"
    networks:
      - observability
      - backend
    <<: *common-deploy
    profiles: ["observability"]

  grafana:
    image: grafana/grafana:11.1.3
    container_name: grafana
    environment:
      GF_SECURITY_ADMIN_USER: ${GRAFANA_ADMIN_USER:-admin}
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_ADMIN_PASSWORD:-admin}
      GF_SERVER_ROOT_URL: ${GRAFANA_ROOT_URL:-http://localhost:3000}
    volumes:
      - grafana_data:/var/lib/grafana
      - ./observability/grafana/provisioning:/etc/grafana/provisioning:ro
    ports:
      - "3000:3000"
    networks:
      - observability
    <<: *common-deploy
    profiles: ["observability"]

  loki:
    image: grafana/loki:2.9.8
    container_name: loki
    command: ["-config.file=/etc/loki/local-config.yaml"]
    volumes:
      - ./observability/loki-config.yaml:/etc/loki/local-config.yaml:ro
      - loki_data:/loki
    ports:
      - "3100:3100"
    networks:
      - observability
    <<: *common-deploy
    profiles: ["observability"]

  promtail:
    image: grafana/promtail:2.9.8
    container_name: promtail
    command: ["-config.file=/etc/promtail/config.yaml"]
    volumes:
      - ./observability/promtail.yaml:/etc/promtail/config.yaml:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - /var/log:/var/log:ro
    networks:
      - observability
    <<: *common-deploy
    profiles: ["observability"]

  # -------- Доп. инструменты (включайте профилем "extra") -------------------

  flower:
    image: mher/flower:2.0
    container_name: flower
    environment:
      FLOWER_PORT: 5555
      CELERY_BROKER_URL: ${CELERY_BROKER_URL:-redis://redis:6379/3}
    ports:
      - "5555:5555"
    depends_on:
      redis:
        condition: service_healthy
    networks:
      - backend
      - edge
    <<: *common-deploy
    profiles: ["extra"]

  pgadmin:
    image: dpage/pgadmin4:8.7
    container_name: pgadmin
    environment:
      PGADMIN_DEFAULT_EMAIL: ${PGADMIN_EMAIL:-admin@example.com}
      PGADMIN_DEFAULT_PASSWORD: ${PGADMIN_PASSWORD:-admin}
    ports:
      - "5050:80"
    depends_on:
      postgres:
        condition: service_healthy
    networks:
      - backend
      - edge
    <<: *common-deploy
    profiles: ["extra"]
