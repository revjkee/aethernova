# genius-core/learning-engine/policy-networks/model_config.yaml

default_model:
  type: transformer
  hidden_size: 1024
  num_layers: 24
  num_heads: 16
  dropout_rate: 0.1
  activation: gelu
  initializer_range: 0.02
  max_position_embeddings: 2048
  layer_norm_eps: 1e-5
  use_lora: true
  lora:
    rank: 16
    alpha: 32
    dropout: 0.05

training:
  batch_size: 64
  learning_rate: 3e-5
  weight_decay: 0.01
  adam_epsilon: 1e-8
  max_grad_norm: 1.0
  warmup_steps: 10000
  total_training_steps: 500000
  gradient_accumulation_steps: 8
  fp16: true

optimizer:
  type: adamw
  betas: [0.9, 0.999]
  weight_decay: 0.01

scheduler:
  type: cosine_annealing
  warmup_ratio: 0.02
  min_lr_ratio: 1e-6

evaluation:
  eval_steps: 1000
  eval_batch_size: 32
  metrics:
    - perplexity
    - accuracy
    - f1_score

checkpointing:
  save_steps: 5000
  max_checkpoints: 10
  keep_best_only: true

logging:
  log_steps: 100
  log_level: info
  tensorboard: true

environment:
  device: cuda
  seed: 42
  distributed: true
  num_gpus: 8

augmentation:
  token_masking_prob: 0.15
  token_replacement_prob: 0.1

notes: |
  Конфигурация включает поддержку LoRA для параметрической экономии и
  более быстрой адаптации моделей к новым задачам. Максимальная длина 
  последовательности 2048 подходит для сложных RL и NLP задач.
