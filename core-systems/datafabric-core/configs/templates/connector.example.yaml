# datafabric-core/configs/templates/connector.example.yaml
# Единый промышленный шаблон одного коннектора Datafabric Core.
# Поддерживает ENV-переменные вида ${VAR} или ${VAR:-default}.
# Рекомендуется хранить секреты в Kubernetes Secret/ENV и не коммитить их в git.

apiVersion: datafabric/v1
kind: Connector
metadata:
  id: "CHANGE_ME"                       # уникальный ID коннектора
  owner: "team-datafabric@company.example"
  description: "Шаблон коннектора: краткая цель и зона ответственности"
  environment: "${ENVIRONMENT:-prod}"
  labels:
    app: "datafabric-core"
    env: "${ENVIRONMENT:-prod}"

spec:
  type: "CHANGE_ME"                     # postgres|kafka|redis|s3|http|grpc|clickhouse|opensearch|...
  enabled: true

  # Универсальные поля для всех типов
  timeouts:
    connectMs: 3000
    readMs: 5000
    writeMs: 5000
    requestMs: 10000
  retry:
    enabled: true
    maxAttempts: 6
    baseDelayMs: 100
    maxDelayMs: 5000
    jitter: true
    retryOn: [ "timeout", "connection_reset", "deadlock", "429", "5xx" ]
  circuitBreaker:
    enabled: true
    failureRateThresholdPct: 50
    slowCallRateThresholdPct: 50
    slowCallDurationMs: 2000
    slidingWindowSize: 50
    permittedCallsInHalfOpen: 5
    waitDurationInOpenStateMs: 30000
  tls:
    enabled: true
    insecureSkipVerify: false
    caFile:  "${TLS_CA_FILE:-}"         # смонтируйте секреты в /etc/datafabric/certs/*
    certFile:"${TLS_CERT_FILE:-}"
    keyFile: "${TLS_KEY_FILE:-}"
    sni:     "${TLS_SNI:-}"
  auth:
    username: "${CONNECTOR_USER:-}"
    password: "${CONNECTOR_PASSWORD:-}"
    token:    "${CONNECTOR_TOKEN:-}"
  pool:
    enabled: true
    maxSize: 32
    minIdle: 2
    maxIdleTimeMs: 60000
    maxLifetimeMs: 1800000

  # Наблюдаемость и политика
  observability:
    metrics:
      enabled: true
      labels:
        connector_id: "${CONNECTOR_ID:-CHANGE_ME}"
    tracing:
      enabled: true
  healthcheck:
    enabled: true
    intervalMs: 10000
    timeoutMs: 2000
    initialDelayMs: 3000
  limits:
    requestBytesMax: 10485760
    perSecondRPS: 2000
    burstRPS: 400
  serialization:
    format: "json"                      # json|avro|protobuf|raw
    avro:
      schemaRegistryUrl: "${SCHEMA_REGISTRY_URL:-}"
      basicAuth:
        username: "${SCHEMA_REGISTRY_USER:-}"
        password: "${SCHEMA_REGISTRY_PASS:-}"

  # ----- Типоспецифичные поля — задаются из шаблонов ниже -----
  # example:
  # postgres:
  #   dsn: "postgresql://user:pass@host:5432/db?sslmode=require"
  # kafka:
  #   brokers: "k1:9093,k2:9093"
  #   security: {...}
  # s3:
  #   endpoint: ""
  #   bucket: "bucket-name"
  #   region: "eu-central-1"
  # http:
  #   baseUrl: "https://api.example.com"
  # grpc:
  #   endpoint: "svc.namespace.svc:50051"

# ------------------------------------------------------------------------------
# ШАБЛОНЫ ДЛЯ РАЗНЫХ ТИПОВ КОННЕКТОРОВ (используйте YAML анкоры)
# ------------------------------------------------------------------------------

x-templates:

  postgres: &tpl_postgres
    type: "postgres"
    postgres:
      dsn: "postgresql://${PG_USER:-app}:${PG_PASSWORD:-change_me}@${PG_HOST:-postgres}.${PG_NS:-datafabric}.svc:${PG_PORT:-5432}/${PG_DB:-app}?sslmode=${PG_SSLMODE:-require}"
      pool:
        maxSize: ${PG_POOL_MAX:-32}
        minIdle: ${PG_POOL_MIN:-2}
      statementTimeoutMs: 5000
      healthcheck:
        sql: "SELECT 1"

  kafka: &tpl_kafka
    type: "kafka"
    kafka:
      brokers: "${KAFKA_BROKERS:-kafka-0.kafka:9093,kafka-1.kafka:9093}"
      security:
        protocol: "${KAFKA_SECURITY_PROTOCOL:-SASL_SSL}"   # PLAINTEXT|SSL|SASL_SSL
        sasl:
          mechanism: "${KAFKA_SASL_MECH:-SCRAM-SHA-512}"
          username: "${KAFKA_SASL_USER:-}"
          password: "${KAFKA_SASL_PASS:-}"
        tls:
          enabled: true
          caFile: "${KAFKA_CA_FILE:-}"
          certFile: "${KAFKA_CERT_FILE:-}"
          keyFile: "${KAFKA_KEY_FILE:-}"
      topics:
        main: "${KAFKA_TOPIC_EVENTS:-df.events}"
        dlq:  "${KAFKA_TOPIC_DLQ:-df.dlq}"
      producer:
        acks: "all"
        idempotent: true
        compression: "lz4"              # lz4|snappy|zstd
        lingerMs: 5
        batchBytes: 1048576
      consumer:
        groupId: "${KAFKA_GROUP_ID:-df-core}"
        sessionTimeoutMs: 10000
        autoOffsetReset: "latest"       # earliest|latest

  redis: &tpl_redis
    type: "redis"
    redis:
      mode: "standalone"                # standalone|cluster|sentinel
      url: "${CACHE_REDIS_URL:-redis://redis.cache.svc:6379/0}"
      tls:
        enabled: ${CACHE_REDIS_TLS_ENABLED:-false}
        caFile: "${CACHE_REDIS_CA_FILE:-}"
      pool:
        maxSize: ${CACHE_POOL_MAX:-64}
      healthcheck:
        command: "PING"

  s3: &tpl_s3
    type: "s3"
    s3:
      endpoint: "${S3_ENDPOINT:-}"      # пусто для AWS
      bucket:   "${S3_BUCKET:-datafabric}"
      region:   "${AWS_REGION:-eu-central-1}"
      pathStyle: ${S3_PATH_STYLE:-false}
      auth:
        accessKeyId:     "${AWS_ACCESS_KEY_ID:-}"
        secretAccessKey: "${AWS_SECRET_ACCESS_KEY:-}"
        sessionToken:    "${AWS_SESSION_TOKEN:-}"
      upload:
        acl: "private"
        sse: "${S3_SSE:-AES256}"        # AES256|aws:kms
        kmsKeyId: "${S3_KMS_KEY_ID:-}"
        multipart:
          enabled: true
          partSizeMb: 16

  http: &tpl_http
    type: "http"
    http:
      baseUrl: "${HTTP_BASE_URL:-https://api.example.com}"
      headers:
        Authorization: "Bearer ${HTTP_TOKEN:-}"
        Content-Type: "application/json"
      timeouts:
        connectMs: 1500
        requestMs: 4000
      retry:
        enabled: true
        maxAttempts: 5
        baseDelayMs: 200
        retryOn: [ "timeout", "429", "5xx" ]

  grpc: &tpl_grpc
    type: "grpc"
    grpc:
      endpoint: "${GRPC_ENDPOINT:-ml-infer.ml.svc:50051}"
      tls:
        enabled: true
        caFile: "${GRPC_CA_FILE:-}"
      deadlinesMs:
        unary: 2000
        stream: 10000

  opensearch: &tpl_opensearch
    type: "opensearch"
    opensearch:
      endpoints: "${OS_ENDPOINTS:-https://opensearch.logging.svc:9200}"
      index: "${OS_INDEX:-datafabric-logs-%{+yyyy.MM.dd}}"
      auth:
        username: "${OS_USER:-}"
        password: "${OS_PASSWORD:-}"
      tls:
        enabled: true
        insecureSkipVerify: false
      timeouts:
        requestMs: 15000

# ------------------------------------------------------------------------------
# ПРИМЕРЫ ИНСТАНЦИЙ (скопируйте нужный блок и поменяйте metadata/spec как требуется)
# ------------------------------------------------------------------------------

examples:

  pg_main:
    metadata:
      id: "pg_main"
      owner: "platform-db@company.example"
      description: "Основная транзакционная БД"
      environment: "${ENVIRONMENT:-prod}"
    spec:
      <<: *tpl_postgres
      enabled: true

  kafka_bus:
    metadata:
      id: "kafka_bus"
      owner: "platform-streams@company.example"
      description: "Шина событий приложения"
    spec:
      <<: *tpl_kafka
      enabled: true

  redis_cache:
    metadata:
      id: "redis_cache"
      owner: "platform-cache@company.example"
      description: "Горячий кэш"
    spec:
      <<: *tpl_redis
      enabled: true

  s3_storage:
    metadata:
      id: "s3_storage"
      owner: "platform-storage@company.example"
      description: "Объектное хранилище для артефактов"
    spec:
      <<: *tpl_s3
      enabled: true

  http_audit_sink:
    metadata:
      id: "http_audit_sink"
      owner: "security@company.example"
      description: "Вынос аудита во внешний сервис"
    spec:
      <<: *tpl_http
      enabled: true

  grpc_inference:
    metadata:
      id: "grpc_inference"
      owner: "ml-team@company.example"
      description: "Онлайновое ML‑скоринг gRPC"
    spec:
      <<: *tpl_grpc
      enabled: false

# ------------------------------------------------------------------------------
# ВАЛИДАЦИЯ (используется приложением при загрузке)
# ------------------------------------------------------------------------------

validation:
  requiredFields:
    - "metadata.id"
    - "spec.type"
  requiredForProd:
    - "spec.tls.enabled=true"
    - "spec.retry.enabled=true"
  forbidInProd:
    - "spec.tls.insecureSkipVerify=true"
  schemaHints:
    postgres:
      mustHave: [ "spec.postgres.dsn" ]
    kafka:
      mustHave: [ "spec.kafka.brokers" ]
    s3:
      mustHave: [ "spec.s3.bucket", "spec.s3.region" ]
