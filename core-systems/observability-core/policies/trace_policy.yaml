apiVersion: tracing.teslaai.org/v1
kind: TracePolicy
metadata:
  name: teslaai-core-trace-policy
  namespace: observability
  labels:
    environment: production
    scope: global
    team: telemetry
    layer: tracing

spec:
  globalDefaults:
    enabled: true
    exporter: otlp
    samplingRate: 0.10          # 10% по умолчанию
    maxSpanDepth: 64
    traceRetention: 7d
    redactSensitiveData: true
    includeLogs: false
    correlationIdHeader: X-TeslaAI-Trace-Id
    propagation:
      - tracecontext
      - b3
      - baggage

  exporters:
    otlp:
      endpoint: "http://otel-collector.monitoring.svc:4318"
      protocol: http/protobuf
      timeout: 5s
      compression: gzip
      headers:
        x-project-tag: "TeslaAI-Genesis"

  selectorRules:
    - name: critical-path-services
      match:
        services:
          - ai-core
          - agent-manager
          - latency-tracker
          - gateway
      overrides:
        samplingRate: 1.0
        includeLogs: true
        traceRetention: 14d
        maxSpanDepth: 128

    - name: edge-services-low-volume
      match:
        services:
          - mobile-app
          - graphql-proxy
        overrides:
          samplingRate: 0.2
          includeLogs: false

    - name: exclude-noise
      match:
        tags:
          - debug
          - healthcheck
      overrides:
        enabled: false

    - name: ai-sensitive-ops
      match:
        tags:
          - "model_inference"
          - "prompt_chain"
        overrides:
          redactSensitiveData: true
          traceRetention: 3d
          samplingRate: 0.5

  fallback:
    enabled: true
    bufferFile: /var/log/trace-buffer.jsonl
    maxBufferSizeMB: 100
    uploadOnRecovery: true
    flushIntervalSeconds: 60
