# filepath: cybersecurity-core/tests/integration/test_ids_suricata.py
# -*- coding: utf-8 -*-
"""
Integration tests for Suricata EVE JSON ingestion pipeline.

Two execution modes:
1) LIVE mode: If SURICATA_INGEST_URL is set (e.g., http://localhost:8080/ingest/suricata),
   tests will POST events over HTTP and validate server responses.
2) OFFLINE mode: If no URL is set, a local simulator is used to validate logic:
   - JSON schema conformance (subset relevant to alert/flow/dns/tls)
   - severity mapping (1->CRITICAL/HIGH, 2->MEDIUM, 3->LOW; see mapping below)
   - deduplication by stable hash
   - idempotent re-ingestion
   - batch streaming handling

Env:
  SURICATA_INGEST_URL   optional, triggers LIVE mode when set
  TEST_EVENT_COUNT      optional, default 2000 for big batch test

Dependencies (tests):
  pip install pytest jsonschema requests

Note:
  Tests avoid external IOCs and network calls unless LIVE mode is enabled.
"""

from __future__ import annotations

import hashlib
import json
import os
import time
from dataclasses import dataclass
from datetime import datetime, timezone
from typing import Any, Dict, Iterable, List, Optional, Tuple

import pytest

try:
    import requests  # type: ignore
except Exception:
    requests = None  # type: ignore

from jsonschema import Draft7Validator

# --------------------------
# Config / Modes
# --------------------------

INGEST_URL = os.getenv("SURICATA_INGEST_URL", "").strip()
LIVE_MODE = bool(INGEST_URL)

# --------------------------
# Minimal EVE JSON Schemas
# --------------------------

EVE_ALERT_SCHEMA = {
    "type": "object",
    "required": ["timestamp", "event_type", "alert"],
    "properties": {
        "timestamp": {"type": "string"},
        "event_type": {"type": "string", "enum": ["alert"]},
        "in_iface": {"type": "string"},
        "src_ip": {"type": "string"},
        "src_port": {"type": "integer"},
        "dest_ip": {"type": "string"},
        "dest_port": {"type": "integer"},
        "proto": {"type": "string"},
        "flow_id": {"type": ["integer", "number", "string"]},
        "alert": {
            "type": "object",
            "required": ["signature", "category", "severity", "gid", "signature_id"],
            "properties": {
                "signature": {"type": "string"},
                "category": {"type": "string"},
                "severity": {"type": "integer", "minimum": 1, "maximum": 3},
                "gid": {"type": "integer"},
                "signature_id": {"type": "integer"},
                "rev": {"type": ["integer", "null"]},
                "metadata": {"type": ["object", "array", "null"]},
            },
            "additionalProperties": True,
        },
    },
    "additionalProperties": True,
}

EVE_FLOW_SCHEMA = {
    "type": "object",
    "required": ["timestamp", "event_type", "flow"],
    "properties": {
        "timestamp": {"type": "string"},
        "event_type": {"type": "string", "enum": ["flow"]},
        "src_ip": {"type": "string"},
        "dest_ip": {"type": "string"},
        "proto": {"type": "string"},
        "flow": {"type": "object"},
    },
    "additionalProperties": True,
}

EVE_DNS_SCHEMA = {
    "type": "object",
    "required": ["timestamp", "event_type", "dns"],
    "properties": {
        "timestamp": {"type": "string"},
        "event_type": {"type": "string", "enum": ["dns"]},
        "src_ip": {"type": "string"},
        "dest_ip": {"type": "string"},
        "dns": {"type": "object"},
    },
    "additionalProperties": True,
}

EVE_TLS_SCHEMA = {
    "type": "object",
    "required": ["timestamp", "event_type", "tls"],
    "properties": {
        "timestamp": {"type": "string"},
        "event_type": {"type": "string", "enum": ["tls"]},
        "src_ip": {"type": "string"},
        "dest_ip": {"type": "string"},
        "tls": {"type": "object"},
    },
    "additionalProperties": True,
}

V_ALERT = Draft7Validator(EVE_ALERT_SCHEMA)
V_FLOW = Draft7Validator(EVE_FLOW_SCHEMA)
V_DNS = Draft7Validator(EVE_DNS_SCHEMA)
V_TLS = Draft7Validator(EVE_TLS_SCHEMA)

# --------------------------
# Helpers / Data Generation
# --------------------------

def utcnow_iso() -> str:
    return datetime.now(timezone.utc).isoformat()


def severity_numeric_to_text(n: int) -> str:
    """
    Suricata numeric severity: 1 (Highest), 2, 3 (Lowest).
    We map to canonical buckets useful for SIEM triage.
    """
    if n == 1:
        return "HIGH"
    if n == 2:
        return "MEDIUM"
    if n == 3:
        return "LOW"
    return "LOW"


def stable_event_hash(ev: Dict[str, Any]) -> str:
    """
    Stable hash for deduplication â€” consider typical keys for uniqueness.
    """
    key = {
        "event_type": ev.get("event_type"),
        "flow_id": str(ev.get("flow_id", "")),
        "src_ip": ev.get("src_ip"),
        "src_port": ev.get("src_port"),
        "dest_ip": ev.get("dest_ip"),
        "dest_port": ev.get("dest_port"),
        "proto": ev.get("proto"),
        "alert_sig": (ev.get("alert", {}) or {}).get("signature"),
        "alert_sid": (ev.get("alert", {}) or {}).get("signature_id"),
        "timestamp": ev.get("timestamp"),
    }
    raw = json.dumps(key, sort_keys=True, ensure_ascii=False)
    return hashlib.sha256(raw.encode("utf-8")).hexdigest()


def make_alert(
    *,
    sig: str,
    category: str = "Potentially Bad Traffic",
    severity: int = 2,
    src_ip: str = "10.0.0.5",
    src_port: int = 54321,
    dest_ip: str = "10.0.0.10",
    dest_port: int = 80,
    proto: str = "TCP",
    gid: int = 1,
    signature_id: int = 2024001,
    flow_id: int = 123456789,
) -> Dict[str, Any]:
    return {
        "timestamp": utcnow_iso(),
        "event_type": "alert",
        "flow_id": flow_id,
        "src_ip": src_ip,
        "src_port": src_port,
        "dest_ip": dest_ip,
        "dest_port": dest_port,
        "proto": proto,
        "alert": {
            "signature": sig,
            "category": category,
            "severity": severity,
            "gid": gid,
            "signature_id": signature_id,
            "rev": 1,
        },
        "in_iface": "eth0",
    }


def make_flow(
    *,
    src_ip: str = "10.0.0.5",
    dest_ip: str = "10.0.0.10",
    proto: str = "TCP",
    flow_id: int = 987654321,
) -> Dict[str, Any]:
    return {
        "timestamp": utcnow_iso(),
        "event_type": "flow",
        "flow_id": flow_id,
        "src_ip": src_ip,
        "dest_ip": dest_ip,
        "proto": proto,
        "flow": {"pkts_toserver": 10, "pkts_toclient": 8},
    }


def make_dns(
    *,
    src_ip: str = "10.0.0.5",
    dest_ip: str = "1.1.1.1",
    rname: str = "example.com",
) -> Dict[str, Any]:
    return {
        "timestamp": utcnow_iso(),
        "event_type": "dns",
        "src_ip": src_ip,
        "dest_ip": dest_ip,
        "dns": {"type": "query", "rrname": rname, "rcode": "NOERROR"},
    }


def make_tls(
    *,
    src_ip: str = "10.0.0.5",
    dest_ip: str = "10.0.0.10",
    sni: str = "app.internal",
) -> Dict[str, Any]:
    return {
        "timestamp": utcnow_iso(),
        "event_type": "tls",
        "src_ip": src_ip,
        "dest_ip": dest_ip,
        "tls": {"sni": sni, "subject": "CN=app.internal", "issuerdn": "CN=ca"},
    }


# --------------------------
# OFFLINE Simulator
# --------------------------

@dataclass
class IngestResult:
    accepted: int
    deduplicated: int
    errors: int
    inserted_hashes: List[str]


class LocalIngestSimulator:
    """
    Minimal ingestion simulator:
      - Validates schema per event_type
      - Applies severity mapping for alert
      - Deduplicates by stable hash
      - Accumulates inserted hashes
    """
    def __init__(self) -> None:
        self._store: set[str] = set()

    def ingest(self, events: Iterable[Dict[str, Any]]) -> IngestResult:
        accepted = 0
        deduped = 0
        errors = 0
        inserted_hashes: List[str] = []
        for ev in events:
            try:
                et = ev.get("event_type")
                if et == "alert":
                    V_ALERT.validate(ev)
                    sev_num = (ev.get("alert") or {}).get("severity", 3)
                    ev["computed_severity"] = severity_numeric_to_text(int(sev_num))
                elif et == "flow":
                    V_FLOW.validate(ev)
                elif et == "dns":
                    V_DNS.validate(ev)
                elif et == "tls":
                    V_TLS.validate(ev)
                else:
                    # ignore unknown event types
                    continue

                h = stable_event_hash(ev)
                if h in self._store:
                    deduped += 1
                    continue
                self._store.add(h)
                inserted_hashes.append(h)
                accepted += 1
            except Exception:
                errors += 1
        return IngestResult(accepted=accepted, deduplicated=deduped, errors=errors, inserted_hashes=inserted_hashes)

    def count(self) -> int:
        return len(self._store)


# --------------------------
# LIVE Client (optional)
# --------------------------

def live_ingest(events: List[Dict[str, Any]]) -> Tuple[int, Dict[str, Any]]:
    """
    Send events to external ingestion service.
    Expected server response (example):
      {"accepted": int, "deduplicated": int, "errors": int}
    """
    assert requests is not None, "requests not installed"
    resp = requests.post(INGEST_URL, json=events, timeout=15)
    resp.raise_for_status()
    data = resp.json()
    accepted = int(data.get("accepted", 0))
    return accepted, data


# --------------------------
# Fixtures
# --------------------------

@pytest.fixture(scope="function")
def simulator() -> Optional[LocalIngestSimulator]:
    if LIVE_MODE:
        return None
    return LocalIngestSimulator()


@pytest.fixture(scope="function")
def sample_batch() -> List[Dict[str, Any]]:
    return [
        make_alert(sig="Test Rule A", severity=1, signature_id=100001, flow_id=42),
        make_alert(sig="Test Rule B", severity=2, signature_id=100002, flow_id=43),
        make_flow(),
        make_dns(),
        make_tls(),
    ]


# --------------------------
# Tests
# --------------------------

@pytest.mark.integration
def test_eve_schema_and_timestamp_parse(sample_batch: List[Dict[str, Any]], simulator: Optional[LocalIngestSimulator]) -> None:
    # Validate timestamp format and schema quickly
    for ev in sample_batch:
        ts = ev["timestamp"]
        # Must be parseable ISO-8601 with timezone
        parsed = datetime.fromisoformat(ts.replace("Z", "+00:00"))
        assert parsed.tzinfo is not None, "timestamp must be timezone-aware"

        if ev["event_type"] == "alert":
            V_ALERT.validate(ev)
        elif ev["event_type"] == "flow":
            V_FLOW.validate(ev)
        elif ev["event_type"] == "dns":
            V_DNS.validate(ev)
        elif ev["event_type"] == "tls":
            V_TLS.validate(ev)

    if LIVE_MODE:
        accepted, data = live_ingest(sample_batch)
        assert accepted >= 3, f"Expected >=3 accepted in LIVE mode, got {accepted}. Response: {data}"
    else:
        assert simulator is not None
        res = simulator.ingest(sample_batch)
        assert res.errors == 0
        assert res.accepted == 5
        assert simulator.count() == 5


@pytest.mark.integration
def test_alert_severity_mapping(sample_batch: List[Dict[str, Any]], simulator: Optional[LocalIngestSimulator]) -> None:
    # Only alerts carry numeric severity
    alerts = [e for e in sample_batch if e["event_type"] == "alert"]
    assert len(alerts) >= 2

    if LIVE_MODE:
        # Server should respond with normalized severities or acceptance
        accepted, _ = live_ingest(alerts)
        assert accepted == len(alerts)
    else:
        assert simulator is not None
        res = simulator.ingest(alerts)
        assert res.errors == 0
        # Check computed_severity set by simulator
        mapped = [severity_numeric_to_text(ev["alert"]["severity"]) for ev in alerts]
        # Force simulation to compute
        _ = simulator.ingest([])  # no-op
        # Ensure mapping aligns with function
        for ev in alerts:
            sev_num = ev["alert"]["severity"]
            expected = severity_numeric_to_text(sev_num)
            # Reconstruct with simulator (already added)
            assert expected in ("HIGH", "MEDIUM", "LOW")


@pytest.mark.integration
def test_deduplication_and_idempotency(sample_batch: List[Dict[str, Any]], simulator: Optional[LocalIngestSimulator]) -> None:
    # Duplicate first two events
    dup_batch = sample_batch + sample_batch[:2]

    if LIVE_MODE:
        accepted1, _ = live_ingest(sample_batch)
        accepted2, _ = live_ingest(sample_batch)  # reingest same batch
        # idempotency expectation: second run should accept 0 or only non-duplicated
        assert accepted1 >= accepted2
    else:
        assert simulator is not None
        res1 = simulator.ingest(sample_batch)
        assert res1.errors == 0
        c1 = simulator.count()
        res2 = simulator.ingest(dup_batch)
        c2 = simulator.count()
        # After re-ingest with duplicates, total must not increase by more than length of non-dup tail
        assert c2 == c1 + max(0, len(dup_batch) - len(sample_batch))
        # And dedup should be accounted
        assert res2.deduplicated >= 2


@pytest.mark.integration
def test_big_batch_streaming(simulator: Optional[LocalIngestSimulator]) -> None:
    n = int(os.getenv("TEST_EVENT_COUNT", "2000"))
    big_batch: List[Dict[str, Any]] = []
    # Interleave alert/flow/dns/tls
    for i in range(n):
        big_batch.append(make_alert(sig=f"Rule-{i%17}", severity=(i % 3) + 1, signature_id=150000 + (i % 1000), flow_id=10_000 + i))
        if i % 2 == 0:
            big_batch.append(make_flow(flow_id=20_000 + i))
        if i % 5 == 0:
            big_batch.append(make_dns(rname=f"ex{i}.com"))
        if i % 7 == 0:
            big_batch.append(make_tls(sni=f"svc{i%11}.local"))

    if LIVE_MODE:
        # send in chunks of 500 to simulate streaming ingestion
        chunk = 500
        total_acc = 0
        for j in range(0, len(big_batch), chunk):
            acc, _ = live_ingest(big_batch[j : j + chunk])
            total_acc += acc
            # avoid hammering
            time.sleep(0.02)
        assert total_acc >= n, f"Expected at least all alerts accepted; got {total_acc}"
    else:
        assert simulator is not None
        # simulate streaming chunks
        chunk = 500
        total_before = simulator.count()
        total_acc = 0
        total_err = 0
        for j in range(0, len(big_batch), chunk):
            res = simulator.ingest(big_batch[j : j + chunk])
            total_acc += res.accepted
            total_err += res.errors
        total_after = simulator.count()
        assert total_err == 0
        assert total_after - total_before == total_acc


@pytest.mark.integration
def test_hash_stability(sample_batch: List[Dict[str, Any]]) -> None:
    # Ensure stable hash unchanged across key reordering
    e = sample_batch[0]
    h1 = stable_event_hash(e)
    # Reorder keys by recreating dict
    reordered = json.loads(json.dumps(e, ensure_ascii=False))
    h2 = stable_event_hash(reordered)
    assert h1 == h2, "stable_event_hash must be order-invariant"
