import asyncio
import hashlib
import logging
import re
from typing import List, Dict, Optional, Tuple

import aiohttp
from bs4 import BeautifulSoup

from intel_fetcher.models.exploit_record import ExploitRecord
from intel_fetcher.utils.retry import resilient_fetch
from intel_fetcher.utils.normalizer import normalize_exploit_title, extract_cve_ids

logger = logging.getLogger("ExploitDBScraper")
logging.basicConfig(level=logging.INFO)

BASE_URLS = {
    "exploitdb": "https://www.exploit-db.com",
    "packetstorm": "https://packetstormsecurity.com",
    "metasploit": "https://raw.githubusercontent.com/rapid7/metasploit-framework/master/modules"
}

HEADERS = {
    "User-Agent": "ForgeMind-Scraper/2.0"
}


class ExploitDBScraper:
    def __init__(self):
        self.existing_hashes = set()

    async def scrape(self) -> List[ExploitRecord]:
        async with aiohttp.ClientSession(headers=HEADERS) as session:
            tasks = [
                self._scrape_exploitdb(session),
                self._scrape_packetstorm(session),
                self._scrape_metasploit(session)
            ]
            all_results = await asyncio.gather(*tasks)
            flat_list = [record for sublist in all_results for record in sublist]
            return flat_list

    async def _scrape_exploitdb(self, session: aiohttp.ClientSession) -> List[ExploitRecord]:
        url = f"{BASE_URLS['exploitdb']}/exploits"
        logger.info(f"Fetching Exploit-DB list from {url}")
        html = await resilient_fetch(session, url)
        if not html:
            return []

        soup = BeautifulSoup(html, "html.parser")
        entries = soup.select("table#exploits-table tbody tr")
        results = []

        for entry in entries:
            try:
                cols = entry.find_all("td")
                link = BASE_URLS["exploitdb"] + cols[1].find("a")["href"]
                title = normalize_exploit_title(cols[2].text)
                date = cols[4].text.strip()
                cve_ids = extract_cve_ids(title)

                exploit_text = await resilient_fetch(session, link)
                content_hash = hashlib.sha256(exploit_text.encode()).hexdigest()
                if content_hash in self.existing_hashes:
                    continue
                self.existing_hashes.add(content_hash)

                results.append(ExploitRecord(
                    title=title,
                    cve_ids=cve_ids,
                    source="exploitdb",
                    date=date,
                    url=link,
                    content=exploit_text,
                    sha256=content_hash
                ))
            except Exception as ex:
                logger.warning(f"Failed to parse Exploit-DB row: {ex}")
                continue
        return results

    async def _scrape_packetstorm(self, session: aiohttp.ClientSession) -> List[ExploitRecord]:
        url = f"{BASE_URLS['packetstorm']}/files/tags/exploit/"
        logger.info(f"Fetching PacketStorm exploits from {url}")
        html = await resilient_fetch(session, url)
        if not html:
            return []

        soup = BeautifulSoup(html, "html.parser")
        results = []

        for link_tag in soup.select("dl dt a"):
            try:
                link = BASE_URLS["packetstorm"] + link_tag["href"]
                title = normalize_exploit_title(link_tag.text)
                exploit_text = await resilient_fetch(session, link)
                cve_ids = extract_cve_ids(title)
                content_hash = hashlib.sha256(exploit_text.encode()).hexdigest()
                if content_hash in self.existing_hashes:
                    continue
                self.existing_hashes.add(content_hash)

                results.append(ExploitRecord(
                    title=title,
                    cve_ids=cve_ids,
                    source="packetstorm",
                    date="unknown",
                    url=link,
                    content=exploit_text,
                    sha256=content_hash
                ))
            except Exception as ex:
                logger.warning(f"PacketStorm parse fail: {ex}")
                continue
        return results

    async def _scrape_metasploit(self, session: aiohttp.ClientSession) -> List[ExploitRecord]:
        logger.info(f"Scanning Metasploit module tree at {BASE_URLS['metasploit']}")
        exploit_paths = [
            "exploit/windows/smb/ms17_010_eternalblue.rb",
            "exploit/multi/http/apache_struts2_content_type.rb"
        ]
        results = []

        for path in exploit_paths:
            try:
                url = f"{BASE_URLS['metasploit']}/{path}"
                exploit_text = await resilient_fetch(session, url)
                title = normalize_exploit_title(path)
                cve_ids = extract_cve_ids(exploit_text)
                content_hash = hashlib.sha256(exploit_text.encode()).hexdigest()
                if content_hash in self.existing_hashes:
                    continue
                self.existing_hashes.add(content_hash)

                results.append(ExploitRecord(
                    title=title,
                    cve_ids=cve_ids,
                    source="metasploit",
                    date="n/a",
                    url=url,
                    content=exploit_text,
                    sha256=content_hash
                ))
            except Exception as ex:
                logger.warning(f"Metasploit parse fail: {ex}")
                continue
        return results

