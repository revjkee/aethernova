# omnimind-core/ops/docker/docker-compose.yaml
version: "3.9"

############################
# Anchors / extensions
############################
x-logging: &default-logging
  driver: "json-file"
  options:
    max-size: "10m"
    max-file: "5"

x-secure: &secure
  read_only: true
  security_opt:
    - no-new-privileges:true
  cap_drop: [ "ALL" ]
  tmpfs:
    - /tmp:rw,nosuid,nodev,noexec,size=64m
  user: "1000:1000"

x-common-deploy: &common-deploy
  deploy:
    resources:
      limits:
        cpus: "1.50"
        memory: "1g"
      reservations:
        cpus: "0.25"
        memory: "256m"

x-env: &envfile
  env_file:
    - ../../.env

############################
# Networks & Volumes
############################
networks:
  internal:
    name: omnimind-internal
    internal: true
  public:
    name: omnimind-public

volumes:
  pg_data:
  redis_data:
  minio_data:
  qdrant_data:
  prometheus_data:
  grafana_data:
  app_logs:

############################
# Secrets (file-based)
############################
secrets:
  db_password:
    file: ./secrets/db_password.txt
  jwt_private:
    file: ./secrets/jwt_private.pem
  jwt_public:
    file: ./secrets/jwt_public.pem
  minio_access_key:
    file: ./secrets/minio_access_key.txt
  minio_secret_key:
    file: ./secrets/minio_secret_key.txt

############################
# Services
############################
services:
  api:
    build:
      context: ../../
      dockerfile: Dockerfile
      # предполагается многослойный Dockerfile приложения
    image: omnimind-core:latest
    <<: [ *envfile ]
    environment:
      # пример: переопределения, если нужно
      # DATABASE_URL: postgresql+asyncpg://omni_user:${DB_PASSWORD}@postgres:5432/omnimind
      # REDIS_URL: redis://:$(cat /run/secrets/db_password)@redis:6379/0
      TZ: ${TZ:-Europe/Stockholm}
    secrets:
      - db_password
      - jwt_private
      - jwt_public
    volumes:
      - app_logs:/var/log/omnimind
    ports:
      - "127.0.0.1:8000:8000"        # внешний доступ только с localhost
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      minio:
        condition: service_started
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://localhost:8000/healthz || exit 1"]
      interval: 15s
      timeout: 5s
      retries: 5
      start_period: 25s
    networks:
      - internal
      - public
    logging: *default-logging
    <<: *secure
    <<: *common-deploy
    restart: unless-stopped

  worker:
    image: omnimind-core:latest
    <<: [ *envfile ]
    command: >
      bash -lc "celery -A omnimind_core.worker worker
      --loglevel=INFO --concurrency=${CELERY_CONCURRENCY:-4}"
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "bash -lc 'celery -A omnimind_core.worker inspect ping -d celery@$$HOSTNAME || exit 1'"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 40s
    networks:
      - internal
    logging: *default-logging
    <<: *secure
    <<: *common-deploy
    restart: unless-stopped

  scheduler:
    image: omnimind-core:latest
    <<: [ *envfile ]
    command: >
      bash -lc "celery -A omnimind_core.worker beat
      --loglevel=INFO --pidfile="
    depends_on:
      redis:
        condition: service_healthy
    networks:
      - internal
    logging: *default-logging
    <<: *secure
    <<: *common-deploy
    restart: unless-stopped

  postgres:
    image: pgvector/pgvector:pg16
    environment:
      POSTGRES_DB: omnimind
      POSTGRES_USER: omni_user
      POSTGRES_PASSWORD_FILE: /run/secrets/db_password
      TZ: ${TZ:-Europe/Stockholm}
    secrets:
      - db_password
    volumes:
      - pg_data:/var/lib/postgresql/data
    ports:
      - "127.0.0.1:5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U omni_user -d omnimind -h localhost"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 20s
    networks:
      - internal
    logging: *default-logging
    restart: unless-stopped

  redis:
    image: redis:7-alpine
    command: >
      redis-server --appendonly yes
                   --save 300 10
                   --loglevel notice
    volumes:
      - redis_data:/data
    ports:
      - "127.0.0.1:6379:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 10s
    networks:
      - internal
    logging: *default-logging
    restart: unless-stopped

  minio:
    image: quay.io/minio/minio:RELEASE.2024-10-02T00-00-00Z
    command: server /data --console-address ":9001"
    environment:
      MINIO_ROOT_USER_FILE: /run/secrets/minio_access_key
      MINIO_ROOT_PASSWORD_FILE: /run/secrets/minio_secret_key
      MINIO_REGION_NAME: ${S3_REGION:-us-east-1}
    secrets:
      - minio_access_key
      - minio_secret_key
    volumes:
      - minio_data:/data
    ports:
      - "127.0.0.1:9000:9000"
      - "127.0.0.1:9001:9001"
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://localhost:9000/minio/health/live || exit 1"]
      interval: 15s
      timeout: 5s
      retries: 10
      start_period: 20s
    networks:
      - internal
      - public
    logging: *default-logging
    restart: unless-stopped

  ############################
  # Observability (profile)
  ############################
  otel-collector:
    profiles: [ "observability" ]
    image: otel/opentelemetry-collector:0.104.0
    command: ["--config=/etc/otelcol/config.yaml"]
    volumes:
      - ./observability/otel-collector.yaml:/etc/otelcol/config.yaml:ro
    ports:
      - "127.0.0.1:4317:4317"   # OTLP gRPC
      - "127.0.0.1:4318:4318"   # OTLP HTTP
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://localhost:13133/health/status || exit 1"]
      interval: 15s
      timeout: 5s
      retries: 10
      start_period: 15s
    networks:
      - internal
      - public
    logging: *default-logging
    restart: unless-stopped

  jaeger:
    profiles: [ "observability" ]
    image: jaegertracing/all-in-one:1.57
    environment:
      - COLLECTOR_OTLP_ENABLED=true
    ports:
      - "127.0.0.1:16686:16686"  # UI
      - "127.0.0.1:14250:14250"
      - "127.0.0.1:14268:14268"
    networks:
      - internal
      - public
    logging: *default-logging
    restart: unless-stopped

  prometheus:
    profiles: [ "observability" ]
    image: prom/prometheus:v2.54.1
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--storage.tsdb.retention.time=15d"
    volumes:
      - ./observability/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus_data:/prometheus
    ports:
      - "127.0.0.1:9090:9090"
    networks:
      - internal
      - public
    logging: *default-logging
    restart: unless-stopped

  grafana:
    profiles: [ "observability" ]
    image: grafana/grafana:11.1.0
    environment:
      - GF_AUTH_ANONYMOUS_ENABLED=true
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - grafana_data:/var/lib/grafana
      - ./observability/grafana/provisioning:/etc/grafana/provisioning:ro
    ports:
      - "127.0.0.1:3000:3000"
    depends_on:
      prometheus:
        condition: service_started
    networks:
      - internal
      - public
    logging: *default-logging
    restart: unless-stopped

  ############################
  # Vector DB (profile)
  ############################
  qdrant:
    profiles: [ "vector" ]
    image: qdrant/qdrant:v1.11.0
    environment:
      QDRANT__SERVICE__GRPC_PORT: 6334
      QDRANT__STORAGE__OPTIMIZER_THRESHOLD: 20000
    volumes:
      - qdrant_data:/qdrant/storage
    ports:
      - "127.0.0.1:6333:6333"   # HTTP
      - "127.0.0.1:6334:6334"   # gRPC
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://localhost:6333/ready || exit 1"]
      interval: 15s
      timeout: 5s
      retries: 10
      start_period: 20s
    networks:
      - internal
      - public
    logging: *default-logging
    restart: unless-stopped
