# agent_rl/trading/configs/hyperparams.yaml

# === Общие параметры обучения ===
global:
  episodes: 1000
  steps_per_episode: 500
  eval_interval: 10
  save_interval: 50
  early_stopping_reward: 0.05
  weight_init: xavier_uniform  # kaiming_uniform / normal / default
  gradient_clip_norm: 1.0
  use_amp: true                # mixed-precision training

# === DQN (Deep Q-Network) ===
dqn:
  gamma: 0.99
  learning_rate: 1e-3
  epsilon:
    start: 1.0
    end: 0.05
    decay: 0.995
  batch_size: 64
  buffer_size: 100000
  target_update: 10
  double_q: true
  dueling_architecture: true
  optimizer: adam
  scheduler:
    type: cosine
    warmup_steps: 1000

# === SAC (Soft Actor-Critic) ===
sac:
  gamma: 0.99
  learning_rate: 3e-4
  tau: 0.005
  alpha:
    initial: 0.2
    auto_tune: true
    target_entropy: auto
  batch_size: 256
  buffer_size: 1000000
  actor_lr: 3e-4
  critic_lr: 3e-4
  alpha_lr: 3e-4
  use_soft_update: true
  actor_hidden: [256, 256]
  critic_hidden: [256, 256]

# === Policy Gradient / A2C / PPO ===
pg:
  gamma: 0.99
  learning_rate: 1e-3
  entropy_coef: 0.01
  normalize_advantage: true
  advantage_clip: true
  actor_hidden: [128, 128]
  critic_hidden: [128, 128]
  optimizer: adamw
  scheduler:
    type: exponential
    decay_rate: 0.95
    warmup_steps: 500

# === Meta-Search Space (если запущен tuner) ===
search_space:
  learning_rate: [1e-4, 3e-4, 1e-3]
  gamma: [0.95, 0.99]
  batch_size: [64, 128, 256]
  tau: [0.001, 0.005, 0.01]

# === Presets ===
presets:
  fast-train:
    episodes: 300
    steps_per_episode: 200
    scheduler.type: linear
  low-risk:
    epsilon.end: 0.2
    alpha.initial: 0.1
    max_drawdown: 0.1
  long-horizon:
    episodes: 5000
    scheduler.type: cosine
    gamma: 0.999

# === Copilot интеграция ===
copilot:
  use: true
  mode: refine_loss
  refine_loss_weight: 0.2
  prompt_embedding_feedback: true
  reward_guided_decision: true
