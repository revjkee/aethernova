# agent_rl/trading/configs/agent_config.yaml

# === Общие параметры агента ===
agent_name: sac_agent                 # dqn_agent / sac_agent / pg_agent / hybrid

device: auto                         # auto / cpu / cuda
random_seed: 42
deterministic: true

# === DQN-специфично ===
dqn:
  hidden_layers: [128, 128]
  gamma: 0.99
  learning_rate: 1e-3
  epsilon_start: 1.0
  epsilon_end: 0.05
  epsilon_decay: 0.995
  target_update_freq: 10
  buffer_size: 100000
  batch_size: 64

# === SAC-специфично ===
sac:
  actor_hidden_layers: [256, 256]
  critic_hidden_layers: [256, 256]
  gamma: 0.99
  tau: 0.005
  alpha: 0.2
  learning_rate: 3e-4
  automatic_entropy_tuning: true
  target_entropy: auto
  buffer_size: 1000000
  batch_size: 256

# === Policy Gradient / A2C / PPO ===
pg:
  actor_hidden_layers: [128, 128]
  critic_hidden_layers: [128, 128]
  gamma: 0.99
  entropy_coef: 0.01
  learning_rate: 1e-3
  advantage_clip: true
  normalize_advantage: true

# === Общие опции поведения ===
behavior:
  use_copilot_llm: false
  copilot_mode: advisory            # advisory / override / none
  fallback_strategy: safe_exit      # fallback on crash

# === Логирование и дебаг ===
logging:
  log_level: INFO                   # DEBUG / INFO / WARNING / ERROR
  log_to_wandb: false
  wandb_project: rl-trading-agents
  log_every_n_steps: 20
